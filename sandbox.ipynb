{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "_Alex Malz, David Mykytyn_\n",
    "\n",
    "This is a sandbox for developing an unsupervised classifier of astronomical lightcurves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ongoing Ideas\n",
    "\n",
    "_Special thanks to David Hogg_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better metric\n",
    "\n",
    "- make it chisquare of arclength\n",
    "- arclength must be relative to reference to account for different noise levels, or arclength relative to sum of both originals, then inverse transformation for symmetry, relative to null of no-overlap\n",
    "\n",
    "complete sine/cosine basis -- rotation into space will look like linear fit, penalize high amplitude on high frequency modes --> probability, prior on frequency or number of modes, that would be a posterior probability\n",
    "\n",
    "linearly interpolate one and get probability of other under same model\n",
    "\n",
    "minimize 2nd derivative/sharpness\n",
    "\n",
    "try different noise levels\n",
    "\n",
    "try crazy outlier points, try arclength over all droppings of single point\n",
    "\n",
    "many bands -- share shift but stretch will differ, can make a prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "- always looking at 3 year segment centered on overlap so never penalized by too long, enforce that there is overlap\n",
    "- prior to make product of stretches = 1\n",
    "\n",
    "chunks?\n",
    "\n",
    "trigger time? probably not provided\n",
    "\n",
    "regularizing erros: probably happens in space of variance, should be sensitive to shift in y, stretch in y, just need ratio of initial and final y values and initial variances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "check symmetry requirements of distance metrics for clustering, i.e. tSNE needs convexity and symmetry\n",
    "\n",
    "hierarchical clustering by combining best fits, reverse merger tree \"active learning\", both way branching is logN so we can visually monitor as we add new things\n",
    "\n",
    "need a way to split out again, add one new thing, check against everything elses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import itertools\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.stats as sps\n",
    "import scipy.optimize as spo\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate some mock data\n",
    "\n",
    "We may need to preprocess to keep it reasonable, constraints on delta/stretch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kinds of LC shapes might we have to worry about physically?  For now, just one transient (Gaussian) and one variable (sinusoid).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gauss(scale, loc=0., amp=1., const=0.):\n",
    "    func = sps.norm(loc, scale)\n",
    "    out = lambda x: amp * func.pdf(x) + const\n",
    "    return out\n",
    "\n",
    "def make_sine(period, phase=0., amp=1., const=0.):\n",
    "    func = lambda x: amp * (np.sin(period * x + phase)) + const\n",
    "    return func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lightcurves are sampled irregularly/sparsely in x and have observational errors/noise on y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cadence(x, scatter):\n",
    "    assert(np.all((x[1:]-x[:-1]) > scatter))\n",
    "    jitter = (np.random.uniform(np.shape(x)) - 0.5) * scatter * 2.\n",
    "    perturbed = x + jitter\n",
    "    return perturbed\n",
    "\n",
    "def noisify_obs(y, scatter):\n",
    "    errs = scatter * np.ones_like(y)\n",
    "    new_y = y + sps.norm(0., scatter).rvs(np.shape(y))\n",
    "    return(new_y, errs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, ready to make some data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_cadence = np.arange(0., 200., 5.)\n",
    "\n",
    "gmodel = make_gauss(10., 100., 50., 1.)\n",
    "gtimes = make_cadence(def_cadence, 0.5)\n",
    "gphot, gerr = noisify_obs(gmodel(gtimes), 0.1)\n",
    "\n",
    "smodel = make_sine(20., 0., 5., 5.)\n",
    "stimes = make_cadence(def_cadence, 0.5)\n",
    "sphot, serr = noisify_obs(smodel(stimes), 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a lightcurve object for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now, limit to 2D: amplitude/flux/magnitude/color vs. time\n",
    "LC = namedtuple('LC', ('x', 'y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glc = LC(gtimes, gphot)\n",
    "slc = LC(stimes, sphot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does it look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(glc.x, glc.y, yerr=gerr, linestyle='None', marker='o', label='Transient')\n",
    "plt.errorbar(slc.x, slc.y, yerr=serr, linestyle='None', marker='+', label='Variable')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The overall strategy\n",
    "\n",
    "We want to test the hypothesis that two lightcurves are noisy/sparse/irregular observations of the same object, under some permitted (afine) transformations.  Then we want to do clustering in the space of goodness-of-fit/consistency measure and the parameters of those transformations to identify classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(lca, lcb):\n",
    "#     minx, maxx = max(min(lca.x), min(lcb.x)), min(max(lca.x), max(lcb.x))\n",
    "    new_x = np.concatenate((lca.x, lcb.x))\n",
    "    new_y = np.concatenate((lca.y, lcb.y))\n",
    "    order = np.argsort(new_x)\n",
    "    ord_x = new_x[order]\n",
    "    ord_y = new_y[order]\n",
    "#     condition = np.where(np.logical_and(ord_x <= maxx, ord_x >= minx))\n",
    "#     ord_x = ord_x[condition]\n",
    "#     ord_y = ord_y[condition]\n",
    "    return LC(ord_x, ord_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regularize in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def regx(lca0, lcb0, lcc):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permitted transformations\n",
    "\n",
    "* shiftx\n",
    "* stretchx\n",
    "* shifty\n",
    "* stretchy\n",
    "* (cross-talk between bands)\n",
    "\n",
    "\n",
    "also adjust error bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#affine transformations: translation vector, scaling/shear matrix, haven't propagated this yet\n",
    "aff = namedtuple('aff', ('t', 's'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(lc, deltax, deltay, stretchx, stretchy):\n",
    "    new_x = (stretchx * lc.x) + deltax\n",
    "    new_y = (stretchy * lc.y) + deltay\n",
    "    return LC(new_x, new_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce to summary statistics (consistency metric)\n",
    "\n",
    "Contenders:\n",
    "\n",
    "* periodogram -- identify periodicity and stochastic noise levels, still okay to initially divide transient from variable\n",
    "* flux per time bins -- trends keeping bin size constant but changing bin ends, i.e. moving window\n",
    "* abs/percent change in color and total flux/magnitude\n",
    "\n",
    "find MAP/MLE of p(A = B | lc_A, lc_B)\n",
    "marginalize over shift/stretch params\n",
    "\n",
    "Regularization is going to be really hard!\n",
    "\n",
    "connect the dots is taking an arc length\n",
    "\n",
    "could use the gaussian error bars to get probability that new hypothesis point lies on original line?\n",
    "\n",
    "Random ideas: gaussian process kernels based on training set, get probability that connect-the-dots is drawn from that distribution; but, we know training set will be biased/incomplete, so will have to be able to adapt kernel, use some kind of exporation of space of kernels (gradient descent, genetic algorithm, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the arclength.  If the two lightcurves came from the same object, then their arclengths should be comparable to one another (if over the same range).  Merging the lightcurves should not significantly affect the total arclength.  We could also consider an area between curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_the_dots(lc):\n",
    "    x_difs = (lc.x[1:] - lc.x[:-1])\n",
    "    y_difs = lc.y[1:] - lc.y[:-1]\n",
    "    sol = np.sqrt(x_difs ** 2 + y_difs ** 2)\n",
    "    return np.sum(sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try doing an optimization to find the transformation parameters minimizing the arclength ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_prob(lca, lcb, ivals=(0., 0., 1., 1.)):\n",
    "    \n",
    "    origa = connect_the_dots(lca)\n",
    "    origb = connect_the_dots(lcb)\n",
    "#     xrangea = np.max(lca.x)-np.min(lca.x)\n",
    "#     yrangea = np.max(lca.y)-np.min(lca.y)\n",
    "#     xrangeb = np.max(lcb.x)-np.min(lcb.x)\n",
    "#     yrangeb = np.max(lcb.y)-np.min(lcb.y)\n",
    "#     print (xrangea,yrangea,xrangeb,yrangeb)\n",
    "\n",
    "    \n",
    "#     xdifmaxa = np.max(lca.x[1:]-lca.x[:-1])\n",
    "#     xdifmina = np.min(lca.x[1:]-lca.x[:-1])    \n",
    "#     ydifmaxa = np.max(np.abs(lca.y[1:]-lca.y[:-1]))\n",
    "#     ydifmina = np.min(np.abs(lca.y[1:]-lca.y[:-1]))\n",
    "    \n",
    "#     print (xdifmaxa,xdifmina,ydifmaxa,ydifmina)\n",
    "    \n",
    "#     xdifmaxb = np.max(lcb.x[1:]-lcb.x[:-1])\n",
    "#     xdifminb = np.min(lcb.x[1:]-lcb.x[:-1])    \n",
    "#     ydifmaxb = np.max(np.abs(lcb.y[1:]-lcb.y[:-1]))\n",
    "#     ydifminb = np.min(np.abs(lcb.y[1:]-lcb.y[:-1]))\n",
    "    \n",
    "    \n",
    "#     print (xdifmaxb,xdifminb,ydifmaxb,ydifminb)\n",
    "\n",
    "\n",
    "\n",
    "    def dxlim(params):\n",
    "        return xrangea - np.abs(params[0])\n",
    "    def sxlim_hi(params):\n",
    "        return xrangea - params[2] * xdifmaxb\n",
    "    def sxlim_lo(params):\n",
    "        return params[2] * xrangeb - xdifmaxa\n",
    "    def sylim_hi(params):\n",
    "        return yrangea - params[3] * ydifmaxb\n",
    "    def sylim_lo(params):\n",
    "        return params[3] * yrangeb - ydifmaxa\n",
    "\n",
    "    def slim(params):\n",
    "        return params[2:]\n",
    "\n",
    "    constraints = [dxlim, sxlim_hi, sxlim_lo, sylim_hi, sylim_lo, slim]\n",
    "    constraints = [{'type':'ineq','fun':x} for x in constraints]\n",
    "    debug = []\n",
    "    def _helper(params):\n",
    "        (deltax, deltay, stretchx, stretchy) = params\n",
    "        lc = transform(lcb, deltax, deltay, stretchx, stretchy)\n",
    "        new_len = connect_the_dots(lc)\n",
    "        lc_both = merge(lca, lc) \n",
    "        length = connect_the_dots(lc_both)\n",
    "        to_min = length\n",
    "        return(to_min)\n",
    "    \n",
    "    res = spo.minimize(_helper, ivals, method='Nelder-Mead', options={'maxiter':10000})#,constraints=constraints)\n",
    "#     print(res)\n",
    "    tmp = transform(lcb, res.x[0], res.x[1], res.x[2], res.x[3])\n",
    "    fin = merge(lca, tmp)\n",
    "    debug = connect_the_dots(fin)\n",
    "    return(res, debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try just doing this with merging and shifting for now and test it when the lightcurves have the same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gtimes2 = gtimes + 50. * np.ones_like(gtimes)#make_cadence(def_cadence, 0.5)\n",
    "gphot2, gerr2 = gphot, gerr#noisify_obs(gmodel(gtimes2), 0.1)\n",
    "glc2 = LC(gtimes2, gphot2)\n",
    "# glc2 = transform(glc, 5., 0., 1., 1.)\n",
    "plt.plot(glc.x, glc.y, label='original')\n",
    "plt.plot(glc2.x, glc2.y, label='shifted')\n",
    "plt.plot(merge(glc, glc2).x, merge(glc, glc2).y, label='merged')\n",
    "plt.legend()\n",
    "print((connect_the_dots(glc), connect_the_dots(glc2), connect_the_dots(merge(glc, glc2)), connect_the_dots(merge(glc, transform(glc2, -0.5, 0., 1., 1.)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### an alternative to the arclength. . .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import chippr\n",
    "\n",
    "# # prior_sigma = 0.16\n",
    "# # prior_var = np.eye(n_bins)\n",
    "# # for b in range(n_bins):\n",
    "# #     prior_var[b] = 1. * np.exp(-0.5 * (z_mids[b] - z_mids) ** 2 / prior_sigma ** 2)\n",
    "# # l = 1.e-4\n",
    "# # prior_var = prior_var+l*np.identity(n_bins)\n",
    "\n",
    "\n",
    "\n",
    "# a = 1.#amplitude\n",
    "# b = 5.#inverse wavelength\n",
    "# c = 1.e-2#random fluctuations\n",
    "# #     prior_var = np.eye(n_bins)\n",
    "# #     for k in range(n_bins):\n",
    "# #         # print(k)\n",
    "# #         prior_var[k] = a * np.exp(-0.5 * b * (z_mids[k] - z_mids) ** 2)\n",
    "# #     prior_var += c * np.eye(n_bins)\n",
    "\n",
    "# #     prior_mean = log_nz_intp\n",
    "# #     # print('prior dimensions: '+str((np.shape(prior_mean), np.shape(prior_var))))\n",
    "# #     prior = mvn(prior_mean, prior_var)\n",
    "# #     if params['prior_mean'] is 'sample':\n",
    "# #         prior_mean = prior.sample_one()\n",
    "# #         prior = mvn(prior_mean, prior_var)\n",
    "\n",
    "# # a = 1.# / n_bins\n",
    "# # b = 20.#1. / z_difs ** 2\n",
    "# # c = 0.\n",
    "# prior_var = np.eye(len(gtimes))\n",
    "# for k in range(len(gtimes)):\n",
    "#     prior_var[k] = a * np.exp(-0.5 * b * (gtimes[k] - gtimes) ** 2)\n",
    "# prior_var += c * np.identity(len(gtimes))\n",
    "\n",
    "# prior_mean = gphot\n",
    "# prior = chippr.mvn(prior_mean, prior_var)\n",
    "# prior_samples = prior.sample(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.errorbar(gtimes, gphot, color='k', yerr=gerr, linestyle='None', marker='o')\n",
    "# for i in range(7):\n",
    "#     plt.plot(gtimes, prior_samples[i])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try the fitting methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans, debug = find_max_prob(glc, glc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the null hypothesis lightcurve, the test hypothesis lightcurve, transformation of the test hypothesis lightcurve making it comparible with the null hypothesis lightcurve, and merger of best transformation of the test hypothesis lightcurve and the null hypothesis lightcurve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reconstruct(lca, lcb, params, truea='', trueb=''):\n",
    "    (dx, dy, sx, sy) = params\n",
    "#     print(params)\n",
    "    fin = transform(lcb, dx, dy, sx, sy)\n",
    "#     print(fin.x)\n",
    "    plt.plot(lca.x, lca.y, label='reference'+truea)\n",
    "    plt.plot(lcb.x, lcb.y, label='hypothetical'+trueb)\n",
    "    plt.plot(merge(lca, fin).x, merge(lca, fin).y, label='merged')\n",
    "    plt.plot(fin.x, fin.y, label='transformed'+trueb)\n",
    "    plt.title(str(params))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reconstruct(glc, glc2, ans.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do this many times!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the simulation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_obj = 10\n",
    "cls_models = [make_gauss, make_sine]\n",
    "cls_params = [{'scale': 10., 'loc': 100., 'amp': 50., 'const': 1.}, \n",
    "              {'period': 20., 'phase': 0., 'amp': 5., 'const': 5.}]\n",
    "cls_wts = None # even split for now\n",
    "num_cls = len(cls_models)\n",
    "# will need a way to draw model params\n",
    "\n",
    "def_cadence = np.arange(0., 200., 5.)\n",
    "lcs = []\n",
    "truth = np.random.choice(range(num_cls), num_obj, p=cls_wts)\n",
    "ids, inds, cts = np.unique(truth, return_counts=True, return_inverse=True)\n",
    "# print(ids, cts, inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make some lightcurves and record which are of the same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_obj):\n",
    "    times = make_cadence(def_cadence, 0.5)\n",
    "    model = cls_models[ids[inds[i]]](**cls_params[ids[inds[i]]])\n",
    "    phot, err = noisify_obs(model(times), 0.1)\n",
    "    lcs.append(LC(times, phot))\n",
    "    \n",
    "masks = np.zeros((num_cls, num_obj, num_obj))\n",
    "for i in ids:\n",
    "    which_ones = np.where(truth == i)[0]\n",
    "#     print(which_ones)\n",
    "    pairs = np.array(list(itertools.permutations(which_ones, 2))).T\n",
    "#     print(pairs)\n",
    "    masks[i, pairs[0], pairs[1]] += 1\n",
    "    \n",
    "# print(masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's loop over the optimization, comparing pairwise.  We won't worry about skipping duplication yet because we can use it as a null test of whether this is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_pipeline(all_lcs):\n",
    "    how_many = len(all_lcs)\n",
    "    indices = range(how_many)\n",
    "    dump_difs = np.empty((how_many, how_many))\n",
    "    dump_params = []\n",
    "    \n",
    "    for i in indices:\n",
    "        one_set = []\n",
    "        for j in indices:\n",
    "            ans, fin_len = find_max_prob(all_lcs[i], all_lcs[j])\n",
    "#             print(i, j, ans, fin_len)\n",
    "            one_set.append(np.asarray(ans.x))\n",
    "            dump_difs[i][j] = fin_len\n",
    "        dump_params.append(one_set)\n",
    "    dump_params = np.array(dump_params)\n",
    "            \n",
    "    return(dump_params, dump_difs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params, all_difs = mini_pipeline(lcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_obj):\n",
    "    for j in range(num_obj):\n",
    "        print((i, j, all_difs[i][j]))\n",
    "        plot_reconstruct(lcs[i], lcs[j], all_params[i][j], truea=str(truth[i]), trueb=str(truth[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for symmetry -- really thought these would be symmetric. . .\n",
    "plt.matshow(np.sum(masks, axis=0))\n",
    "layered = np.swapaxes(all_params, 0, -1)\n",
    "\n",
    "deltafunc = lambda x: np.abs(x)\n",
    "stretchfunc = lambda x: np.min(np.array([x, 1./x]).T, axis=-1)\n",
    "funcs = [deltafunc, deltafunc, stretchfunc, stretchfunc]\n",
    "\n",
    "for i in range(4):\n",
    "    plt.matshow(funcs[i](layered[i]))\n",
    "    plt.plot([0, num_obj-1], [0, num_obj-1], color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, this doesn't seem to be working well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster in the space of summary statistics\n",
    "\n",
    "kdtree (and more)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see if the stretch/shear parameters for a class are clustered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_mask = np.zeros((num_obj, num_obj))\n",
    "# for i in range(4):\n",
    "#     global_mask = np.logical_or(global_mask, masks[i])\n",
    "for i in range(num_cls):\n",
    "    global_mask = np.logical_or(global_mask, masks[i])\n",
    "    plt.hist((all_difs * masks[i]).flatten(), alpha=0.25, label=str(i))\n",
    "plt.hist(all_difs[~global_mask[i]].flatten(), alpha=0.25, label='no match')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sort of makes sense because we expect pairs of (s, 1/s) for stretch s and (t, -t) for translation t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner.corner(all_params.reshape(100, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch, they really aren't symmetric nor interpretable.  But this is a very small sample size. . . so let's do it a little better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try another approach :-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listerize(data, masks):\n",
    "    datashape = np.shape(data)\n",
    "    global_mask = np.ma.make_mask_none(np.shape(masks)[1:])\n",
    "    layers = []\n",
    "    for i in range(len(masks)):# per class\n",
    "        one_mask = np.ma.make_mask(masks[i])\n",
    "        layer = np.ma.array(data, mask=np.ma.logical_not(one_mask)[np.newaxis])#data * masks[i][np.newaxis]\n",
    "        global_mask = np.ma.mask_or(global_mask, one_mask)\n",
    "        layers.append(layer.compressed())\n",
    "        \n",
    "    global_mask = np.ma.make_mask(global_mask)\n",
    "    others = np.ma.array(data, mask=global_mask[np.newaxis]).compressed()#data * ~global_mask[np.newaxis]\n",
    "    return(layers, others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_class, mismatch = listerize(all_difs, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_estimation(m1, m2):\n",
    "    X, Y = np.mgrid[min(m1):max(m1):100j, min(m2):max(m2):100j]                                                     \n",
    "    positions = np.vstack([X.ravel(), Y.ravel()])                                                       \n",
    "    values = np.vstack([m1, m2])                                                                        \n",
    "    kernel = sps.gaussian_kde(values)                                                             \n",
    "    Z = np.reshape(kernel(positions).T, X.shape)\n",
    "    return X, Y, Z\n",
    "\n",
    "def mycorner(data, keys, colors, maps, lims=None, pre_densities=None, filename='plot.pdf'):\n",
    "    ncol = len(keys)\n",
    "    fig = plt.figure(figsize=(ncol*5, ncol*5))\n",
    "    ax = [[fig.add_subplot(ncol, ncol, ncol * i + j + 1) for j in range(i+1)] for i in range(ncol)]\n",
    "#     print(len(data), len(colors))\n",
    "    for k in range(len(data)):\n",
    "        datum = data[k]\n",
    "        npoints = len(datum)\n",
    "        for i in range(ncol):\n",
    "            for j in range(i+1):\n",
    "                if i == j:\n",
    "#                     print(datum[keys[i]])\n",
    "                    ax[i][j].hist(datum[i].data, histtype='step', linewidth=2, alpha=0.5, color=colors[k])\n",
    "                    ax[i][j].set_xlabel(keys[i])\n",
    "                else:\n",
    "#                     if (npoints >= 1e4 or npoints <= 100):\n",
    "                    ax[i][j].scatter(datum[i].data, datum[j].data, color=colors[k], alpha=0.5)\n",
    "#                     else:\n",
    "#                         if pre_densities is None:\n",
    "#                             x, y, z = density_estimation(datum[keys[i]], datum[keys[j]])\n",
    "#                         else:\n",
    "#                             (x, y, z) = pre_densities[i][j]\n",
    "#                         ax[i][j].contour(x, y, z, cmap=plt.get_cmap(maps[k]) , alpha=0.5)\n",
    "                    ax[i][j].set_xlabel(keys[i])\n",
    "                    ax[i][j].set_ylabel(keys[j])\n",
    "#                     if lims is not None:\n",
    "#                         ax[i][j].set_xlim(lims)\n",
    "#                         ax[i][j].set_ylim(lims)\n",
    "#     fig.savefig(filename, dpi=100)\n",
    "    return#(fig)\n",
    "# replace with 2d histogram for speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print per_class[0].shape, per_class[1].shape\n",
    "print len(mismatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycorner([per_class[0], per_class[1], mismatch], ['deltax', 'deltay', 'stretchx', 'stretchy'], ['r', 'g', 'b'], ['Reds', 'Greens', 'Blues'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope, this still doesn't look like anything. . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_obj):\n",
    "    for j in range(num_obj):\n",
    "        plot_reconstruct(lcs[i], lcs[j], all_params[i][j], truea=str(truth[i]), trueb=str(truth[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other ideas\n",
    "\n",
    "pairwise combinations/comparisons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "space partitioning -- try to estimate\n",
    "\n",
    "$$\\int_{\\theta \\in D} p(x|\\theta) d\\theta$$\n",
    "\n",
    "then we can iteratively refine. maybe have coarse upper/lower bounds by sampling discrete $\\theta$ and multiplying by the volume of $D$, idk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we have independent observations $y_i$ and times $x_i$, maybe we want the product probability that some underlying function $f$ generated those points\n",
    "\n",
    "$$\\prod_i p(y_i | \\theta)$$\n",
    "\n",
    "if the error distributions are just gaussians with standard deviation $\\sigma$ then the logprob is\n",
    "\n",
    "$$\\log \\left(\\prod_i \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2}\\left(\\frac{f(x_i)-y_i}{\\sigma}\\right)^2} \\right)$$\n",
    "\n",
    "which is for some constant $|C|$,\n",
    "\n",
    "$$-|C| \\sum_i \\left(f(x_i) - y_i\\right)^2$$\n",
    "\n",
    "if $f$ is differentiable like the sine function, i think we can get a closed-form solution for finding the min (closest parameters). we probably want something more bayesian, but i imagine the probabilities fall off kinda fast from the min point, so having it as a reference might be nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm as scipy_norm\n",
    "\n",
    "x = np.linspace(scipy_norm.ppf(0.01),\n",
    "                scipy_norm.ppf(0.99), 100)\n",
    "plt.plot(x, scipy_norm.pdf(x, 1, 2),\n",
    "        'r-', lw=5, alpha=0.6, label='norm pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(scipy_norm.ppf(0.01),\n",
    "                scipy_norm.ppf(0.99), 100)\n",
    "plt.plot(x, scipy_norm.pdf((x - 1) / 2),\n",
    "        'r-', lw=5, alpha=0.6, label='norm pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsum_p_obs(expected, observed, noise_scale):\n",
    "    diffs = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
